{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51e2bc0",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "@page {\n",
    "    margin: 1in;\n",
    "    @top-left {\n",
    "        content: \"EICDT24404\";\n",
    "        font-weight: bold;\n",
    "    }\n",
    "    @top-right {\n",
    "        content: \"Deep Learning\";\n",
    "        font-weight: bold;\n",
    "    }\n",
    "    @bottom-left {\n",
    "        content: \"Lab-1\";\n",
    "    }\n",
    "    @bottom-right {\n",
    "        content: \"Enrollment Number: 230090\";\n",
    "    }\n",
    "}\n",
    "\n",
    "body {\n",
    "    font-family: \"Georgia\", \"Times New Roman\", serif;\n",
    "    font-size: 14pt;\n",
    "}\n",
    "</style>\n",
    "\n",
    "# Lab - 1 - Warm-Up: Vectors and Dot Product for Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02201c6",
   "metadata": {},
   "source": [
    "## 1. Aim : The aim of this lab warm-up is to introduce the minimum mathematical and computational foundations required to understand and implement a Perceptron. This warm-up focuses on vectors, dot product, and linear combinations, which form the core computation in all neural networks and deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623055de",
   "metadata": {},
   "source": [
    "#### Exercise 1: Scalars and Vectors\n",
    "#### Objective: Understand how scalars and vectors are represented in NumPy.\n",
    "#### • Define a scalar bias value\n",
    "#### • Define an input vector and a weight vector\n",
    "#### • Display their values\n",
    "#### Observation:\n",
    "#### • Identify which variables represent learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e910ee4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: 0.5\n",
      "Input vector: [  1.    2.    3.   -5.    6.4   2.   34.  199.   29.  504.   20.1  33.3]\n",
      "Weight vector : [ 0.2  0.5  0.3  0.1 -0.4  0.6  0.7 -0.2  0.3  0.8 -0.5  0.9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a scalar bias value\n",
    "b = 0.5\n",
    "\n",
    "# Define an input vector and weight vector\n",
    "x = np.array([1.0, 2.0, 3.0,-5.0, 6.4, 2, 34,199,29,504, 20.1,33.3])\n",
    "\n",
    "# Define a weight vector (learnable parameter)\n",
    "w = np.array([0.2, 0.5, 0.3, 0.1, -0.4, 0.6, 0.7, -0.2, 0.3, 0.8, -0.5, 0.9])\n",
    "\n",
    "# Display their values\n",
    "print(\"Bias:\", b)\n",
    "print(\"Input vector:\", x)\n",
    "print(\"Weight vector :\", w)\n",
    "\n",
    "# Observation: Learnable parameters are w and b, they can be changed during trainings\n",
    "# Input vector x comes from the data and is not learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f9d292",
   "metadata": {},
   "source": [
    "#### Exercise 2: Dot Product\n",
    "#### Objective: Compute the dot product between input and weight vectors.\n",
    "#### • Use NumPy to compute the dot product\n",
    "#### • Print the output\n",
    "#### Interpretation:\n",
    "#### • A positive value indicates the input lies on one side of the decision boundary\n",
    "#### • A negative value indicates the opposite side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b880cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product (w · x): 416.06000000000006\n",
      "The input lies on one side of the decision boundary.\n"
     ]
    }
   ],
   "source": [
    "# DOt product- \n",
    "dot_product = np.dot(w, x)\n",
    "print(\"Dot product (w · x):\", dot_product)\n",
    "\n",
    "# Interpretation\n",
    "if dot_product > 0:\n",
    "    print(\"The input lies on one side of the decision boundary.\")\n",
    "else:\n",
    "    print(\"The input lies on the opposite side of the decision boundary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a865586",
   "metadata": {},
   "source": [
    "#### Exercise 3: Linear Combination\n",
    "#### Objective: Compute the linear output of a neuron.\n",
    "#### • Add bias to the dot product\n",
    "#### • Observe how bias shifts the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eded8b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: z = 416.56000000000006\n",
      "Bias 0.0: z = 416.06000000000006\n",
      "Bias 1.0: z = 417.06000000000006\n",
      "Bias -1.0: z = 415.06000000000006\n",
      "Bias 2.0: z = 418.06000000000006\n"
     ]
    }
   ],
   "source": [
    "def perceptron_output(w, x, b):\n",
    "    dot_product = np.dot(w, x)\n",
    "    z = dot_product + b\n",
    "    return z\n",
    "\n",
    "w1 = np.array([0.2, 0.5, 0.3, 0.1, -0.4, 0.6, 0.7, -0.2, 0.3, 0.8, -0.5, 0.9])\n",
    "b1 = 0.5\n",
    "x1 = np.array([1.0, 2.0, 3.0,-5.0, 6.4, 2, 34,199,29,504, 20.1,33.3])\n",
    "\n",
    "z1 = perceptron_output(w1, x1, b1)\n",
    "print(f\"Original: z = {z1}\")\n",
    "\n",
    "# Checking for different biases\n",
    "biases_demo = [0.0, 1.0, -1.0, 2.0]\n",
    "for bias_val in biases_demo:\n",
    "    z_demo = perceptron_output(w1, x1, bias_val)\n",
    "    print(f\"Bias {bias_val}: z = {z_demo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e58cbe",
   "metadata": {},
   "source": [
    "#### Exercise 4: Step Activation Function\n",
    "#### Objective: Convert linear output into a binary decision.\n",
    "#### • Implement a simple step activation function\n",
    "#### • Apply it to the linear output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981c64ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = 3.8999999999999995, Step output = 1\n",
      "Bias 0.0: z = 3.6999999999999993, Step output = 1\n",
      "Bias 0.5: z = 4.199999999999999, Step output = 1\n",
      "Bias -0.5: z = 3.1999999999999993, Step output = 1\n",
      "Bias 1.0: z = 4.699999999999999, Step output = 1\n"
     ]
    }
   ],
   "source": [
    "def step_activation(z):\n",
    "    return 1 if z >= 0 else 0\n",
    "\n",
    "\n",
    "x_new1 = np.array([0.5, -1.0, 2.0, 1.5, -0.5, 3.0])\n",
    "w_new1 = np.array([0.3, -0.2, 0.4, 0.1, -0.6, 0.7])\n",
    "b_new1 = 0.2\n",
    "\n",
    "\n",
    "z1 = perceptron_output(w_new1, x_new1, b_new1)\n",
    "output1 = step_activation(z1)\n",
    "print(f\"z = {z1}, Step output = {output1}\")\n",
    "\n",
    "\n",
    "biases_test = [0.0, 0.5, -0.5, 1.0]\n",
    "\n",
    "for bias in biases_test:\n",
    "    z_test = perceptron_output(w_new1, x_new1, bias)\n",
    "    step_out = step_activation(z_test)\n",
    "    print(f\"Bias {bias}: z = {z_test}, Step output = {step_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e54c555",
   "metadata": {},
   "source": [
    "#### Discussion Point: Why does a perceptron use an activation function?\n",
    "#### Activation functions introduce non-linearity which helps the perceptron learn complex patterns and not just the linear ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c568f",
   "metadata": {},
   "source": [
    "#### Exercise 5: Matrix as Batch of Samples\n",
    "#### Objective: Understand batch processing.\n",
    "#### • Define a matrix where each row is a data sample\n",
    "#### • Print the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39debbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch matrix X (each row is a sample):\n",
      "[[ 1.   2.   3. ]\n",
      " [ 0.5 -1.   2. ]\n",
      " [-0.5  1.5 -2. ]\n",
      " [ 2.   0.   1. ]]\n",
      "Shape: (4, 3) (rows: samples, columns: features)\n"
     ]
    }
   ],
   "source": [
    "# Define a matrix where each row represents a data sample\n",
    "X_batch = np.array([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [0.5, -1.0, 2.0],\n",
    "    [-0.5, 1.5, -2.0],\n",
    "    [2.0, 0.0, 1.0]   \n",
    "])\n",
    "\n",
    "print(\"Batch matrix X (each row is a sample):\")\n",
    "print(X_batch)\n",
    "print(f\"Shape: {X_batch.shape} (rows: samples, columns: features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4bfe06",
   "metadata": {},
   "source": [
    "#### Exercise 6: Batch Dot Product\n",
    "#### Objective: Apply dot product to multiple samples simultaneously.\n",
    "#### • Compute dot product between matrix and weight vector\n",
    "#### • Add bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b8b9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot products for batch: [ 1.8  1.6 -1.7  1.4]\n",
      "Linear outputs z for batch (after adding bias): [ 1.9  1.7 -1.6  1.5]\n",
      "Step activation outputs for batch: [1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "w_batch = np.array([0.4, -0.2, 0.6])\n",
    "\n",
    "dot_products_batch = X_batch @ w_batch\n",
    "print(\"Dot products for batch:\", dot_products_batch)\n",
    "\n",
    "b_batch = 0.1\n",
    "z_batch = dot_products_batch + b_batch\n",
    "print(\"Linear outputs z for batch (after adding bias):\", z_batch)\n",
    "\n",
    "outputs_batch = np.array([step_activation(z) for z in z_batch])\n",
    "print(\"Step activation outputs for batch:\", outputs_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342c0fd",
   "metadata": {},
   "source": [
    "7. Post-Lab Reflection Questions\n",
    "Answer the following questions in your lab record:\n",
    "\n",
    "1 . What role do weights play in the dot product?\n",
    "- Weights scale each input feature before summation; the dot product computes the weighted sum that determines the neuron's activation. Learning adjusts weights to change how features influence the output and thus reshape the decision boundary.\n",
    "\n",
    "2. How does bias affect the decision boundary?\n",
    "- Bias shifts the activation value (z) up or down, effectively translating the decision boundary without changing the weights. Adjusting bias moves the threshold at which the neuron fires.\n",
    "\n",
    "3. Why is dot product fundamental to neural networks?\n",
    "- The dot product is the core linear operation that combines inputs and weights into a single activation value. It's efficient, vectorizable, and composes naturally into matrix operations for layers and batches, forming the backbone of forward and backward passes.\n",
    "\n",
    "4. How does batch computation help in training deep learning models?\n",
    "\n",
    "- Batch computation processes multiple samples at once using matrix operations, improving computational efficiency (via vectorization and hardware acceleration), providing more stable gradient estimates, and increasing throughput during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
